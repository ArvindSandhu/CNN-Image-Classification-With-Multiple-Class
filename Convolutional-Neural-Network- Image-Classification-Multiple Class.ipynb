{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Image Classification With Multiple Class\n",
    "## Group 5 assignment:\n",
    "                                Arvind Sandhu\n",
    "                                Harshita\n",
    "                                Ritik\n",
    "                                Sughandha\n",
    "                                Nivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing OS and changing working directory\n",
    "import os\n",
    "os.chdir(\"C:/Users/Sandhu/Desktop/Data Sets/Rock-Paper-Scissors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN by creating an object of sequential class\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a convolution layer by using the \"Conv2D\" function\n",
    "#It takes 4 arguments: 1. No. of filters, 2. Shape of each filter 3. Input shape and type of image(RGB or Black and White) of each image\n",
    "#Feed the classifier with 64X64 resolution.\n",
    "#\"3\" stands for RGB i.e. colour image\n",
    "# 4. Activation function. 'relu' to be used as activation function and stands for rectifier function\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start by taking the classifier object and add the pooling layer.\n",
    "# Take a 2x2 matrix to have minimum pixel loss and get a precise region where the feature are located.\n",
    "#Perform Pooling operation on the resultant feature maps after convolution operation is done on an image\n",
    "#Reduce the size of images as much as possible using pooling i.e try to reduce the total\n",
    "\n",
    "#Step2. Pooling th data\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start by taking the classifier object and add the pooling layer.\n",
    "# Take a 2x2 matrix to have minimum pixel loss and get a precise region where the feature are located.\n",
    "#Perform Pooling operation on the resultant feature maps after convolution operation is done on an image\n",
    "#Reduce the size of images as much as possible using pooling i.e try to reduce the total \n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3),activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a third convolutional layer\n",
    "classifier.add(Conv2D(16, (3, 3),activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covert all the pooled images into a continuous vector through Flattening.\n",
    "#The 2-D array of pooled image pixels are to be converted into a one dimensional single vector.\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a fully connected layer to which connect the set of nodes obtained after flattening step.\n",
    "#Nodes after flattening step will act as input layer to the fully connected layers(hidden layer between input and output layers)\n",
    "# 'Dense' is the function to add a fully connected layer\n",
    "#'units' defines the number of nodes that should be present in the hidden layer(value of units will be between no. of input nodes\n",
    "  # and output nodes. Common practice us to use a power of 2.)\n",
    "#' relu' is the activation function and is a rectifier function\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "#Optimizer parameter is to choose the stochastic gradient descent algorithm.\n",
    "#Loss parameter is to choose the loss function.\n",
    "#The metrics parameter is to choose the performance metric.\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2520 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "#use keras.preprocessing library for doing the synthesising part as well as to prepare the training set as well as\n",
    "#the test test set of images that are present in a properly structured directories\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "shear_range = 0.2,\n",
    "zoom_range = 0.2,\n",
    "horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "#Importing the dataset\n",
    "X_train = train_datagen.flow_from_directory('train',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode ='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 372 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "X_test = test_datagen.flow_from_directory('test', \n",
    "                                          target_size=(64, 64), \n",
    "                                          batch_size=32, \n",
    "                                          class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Considerations\n",
    "1.Here we have 2520 images for the training set, which is not enough to avoid over-fitting. So, we perform image augmentation, such as rotating, flipping, or shearing to increase the number of images. \n",
    "\n",
    "2.ImageDataGenerator generate batches of tensor image data with real-time data augmentation.The data will be looped over (in batches).It splits training images into batches, and each batch will be applied random image transformation on a random selection of images, to create many more diverse images.\n",
    "\n",
    "\n",
    "3.rescale' is a rescaling factor. Defaults to None. If None or 0, no rescaling is applied,otherwise we multiply the data by the value provided (after applying all other transformations).\n",
    "\n",
    "4.shear_range is a Float value.i.e. Shear Intensity (Shear angle in counter-clockwise direction in degrees) 4.zoom_range is a Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "78/78 [==============================] - 167s 2s/step - loss: 0.8921 - accuracy: 0.5507 - val_loss: 0.4214 - val_accuracy: 0.8145\n",
      "Epoch 2/10\n",
      "78/78 [==============================] - 90s 1s/step - loss: 0.1539 - accuracy: 0.9494 - val_loss: 0.2417 - val_accuracy: 0.8737\n",
      "Epoch 3/10\n",
      "78/78 [==============================] - 87s 1s/step - loss: 0.0646 - accuracy: 0.9815 - val_loss: 0.3091 - val_accuracy: 0.8790\n",
      "Epoch 4/10\n",
      "78/78 [==============================] - 89s 1s/step - loss: 0.0265 - accuracy: 0.9928 - val_loss: 0.3001 - val_accuracy: 0.8844\n",
      "Epoch 5/10\n",
      "78/78 [==============================] - 90s 1s/step - loss: 0.0165 - accuracy: 0.9961 - val_loss: 0.2980 - val_accuracy: 0.8602\n",
      "Epoch 6/10\n",
      "78/78 [==============================] - 88s 1s/step - loss: 0.0106 - accuracy: 0.9974 - val_loss: 0.3919 - val_accuracy: 0.8199\n",
      "Epoch 7/10\n",
      "78/78 [==============================] - 88s 1s/step - loss: 0.0232 - accuracy: 0.9919 - val_loss: 0.3653 - val_accuracy: 0.8414\n",
      "Epoch 8/10\n",
      "78/78 [==============================] - 88s 1s/step - loss: 0.0125 - accuracy: 0.9963 - val_loss: 0.3430 - val_accuracy: 0.8871\n",
      "Epoch 9/10\n",
      "78/78 [==============================] - 89s 1s/step - loss: 0.0170 - accuracy: 0.9957 - val_loss: 0.2975 - val_accuracy: 0.9005\n",
      "Epoch 10/10\n",
      "78/78 [==============================] - 89s 1s/step - loss: 0.0238 - accuracy: 0.9917 - val_loss: 0.5187 - val_accuracy: 0.8602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a30fb5fe80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, \n",
    "                         steps_per_epoch=2520/32, \n",
    "                         epochs=10, validation_data=X_test, validation_steps=372/32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Considerations\n",
    "1.First, we create a folder ‘single_prediction’ for the images to be predicted \n",
    "2.Second, we use the image module from Keras to load test images. Note, set the target_size of the image to be (64, 64).\n",
    "3.CNN requires an input image with 3 dimensions. So we need to add a dimension for the channel, from 2D array to 3D array.\n",
    "4.CNN expects another dimension for the batch. Axis is to specify the position of the dimension we are adding. So batch dimension is added at index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper': 0, 'rock': 1, 'scissors': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('validation/scissors7.png', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "X_train.class_indices #To know the mapping between animals and their associated numerical values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scissors\n"
     ]
    }
   ],
   "source": [
    "if result[0][0] == 1:\n",
    "    prediction = 'Paper'\n",
    "elif result[0][1] == 1:\n",
    "    prediction = 'Rock'\n",
    "elif result[0][2] == 1:\n",
    "    prediction = 'Scissors'\n",
    "else:\n",
    "    prediction = 'Error'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
